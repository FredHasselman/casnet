% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fd.R
\name{fd_dfa}
\alias{fd_dfa}
\title{Detrended Fluctuation Analysis (DFA)}
\usage{
fd_dfa(
  y,
  fs = NULL,
  removeTrend = c("no", "poly", "adaptive", "bridge")[2],
  polyOrder = 1,
  standardise = c("none", "mean.sd", "median.mad")[2],
  adjustSumOrder = FALSE,
  removeTrendSegment = c("no", "poly", "adaptive", "bridge")[2],
  polyOrderSegment = 1,
  scaleMin = 16,
  scaleMax = stats::nextn(floor(NROW(y)/4), factors = 2),
  scaleResolution = round(log2(scaleMax - scaleMin)),
  dataMin = NA,
  scaleS = NA,
  overlap = NA,
  doPlot = FALSE,
  returnPlot = FALSE,
  returnPLAW = FALSE,
  returnInfo = FALSE,
  silent = FALSE,
  noTitle = FALSE,
  tsName = "y"
)
}
\arguments{
\item{y}{A numeric vector or time series object.}

\item{fs}{Sample rate}

\item{removeTrend}{Method to use for global detrending (default = \code{"poly"})}

\item{polyOrder}{Order of global polynomial trend to remove if \code{removeTrend = "poly"}. If \code{removeTrend = "adaptive"} polynomials \code{1} to \code{polyOrder} will be evaluated and the best fitting curve (R squared) will be removed (default = \code{1})}

\item{standardise}{Standardise the series using \code{\link[=ts_standardise]{ts_standardise()}} with \code{adjustN = FALSE} (default = "mean.sd")}

\item{adjustSumOrder}{Adjust the time series (summation or difference), based on the global scaling exponent, see e.g. \url{https://www.frontiersin.org/files/Articles/23948/fphys-03-00141-r2/image_m/fphys-03-00141-t001.jpg}{Ihlen (2012)} (default = \code{FALSE})}

\item{removeTrendSegment}{Method to use for detrending in the bins (default = \code{"poly"})}

\item{polyOrderSegment}{The DFA order, the order of polynomial trend to remove from the bin if \code{removeTrendSegment = "poly"}. If \code{removeTrendSegment = "adaptive"} polynomials \code{1} to \code{polyOrder} will be evaluated and the best fitting polynomial (R squared) will be removed (default = \code{1})}

\item{scaleMin}{Minimum scale (in data points) to use for log-log regression (default = \code{4})}

\item{scaleMax}{Maximum scale (in data points) to use for log-log regression. This value will be ignored if \code{dataMin} is not \code{NA} (default = \code{stats::nextn(floor(NROW(y)/4), factors = 2)})}

\item{scaleResolution}{The scales at which detrended fluctuation will be evaluated are calculated as: \code{seq(scaleMin, scaleMax, length.out = scaleResolution)} (default =  \code{round(log2(scaleMax-scaleMin))}).
#' @param dataMin Minimum number of data points in a bin required for inclusion in calculation of the scaling relation. For example if \code{length(y) = 1024} and \code{dataMin = 4}, the maximum scale used to calculate the slope will be \code{1024 / 4 = 256}. This value will take precedence over the \code{scaleMax} (default = \code{NA})}

\item{scaleS}{If not \code{NA}, it should be a numeric vector listing the scales on which to evaluate the detrended fluctuations. Arguments \verb{scaleMax, scaleMin, scaleResolution} and \code{dataMin} will be ignored (default = \code{NA})}

\item{overlap}{A number in \verb{[0 ... 1]} representing the amount of 'bin overlap' when calculating the fluctuation. This reduces impact of arbitrary time series begin and end points. If \code{length(y) = 1024} and overlap is \code{.5}, a scale of \code{4} will be considered a sliding window of size \code{4} with step-size \code{floor(.5 * 4) = 2}, so for scale \code{128} step-size will be \code{64} (default = \code{NA})}

\item{doPlot}{Output the log-log scale versus fluctuation plot with linear fit by calling function \code{plotFD_loglog()} (default = \code{TRUE})}

\item{returnPlot}{Return ggplot2 object (default = \code{FALSE})}

\item{returnPLAW}{Return the power law data (default = \code{FALSE})}

\item{returnInfo}{Return all the data used in SDA (default = \code{FALSE})}

\item{silent}{Silent-ish mode (default = \code{FALSE})}

\item{noTitle}{Do not generate a title (only the subtitle) (default = \code{FALSE})}

\item{tsName}{Name of y added as a subtitle to the plot (default = \code{"y"})}
}
\value{
Estimate of Hurst exponent (slope of \code{log(bin)} vs. \verb{log(RMSE))} and an FD estimate based on Hasselman (2013)
A list object containing:
\itemize{
\item A data matrix \code{PLAW} with columns \code{freq.norm}, \code{size} and \code{bulk}.
\item Estimate of scaling exponent \code{sap} based on a fit over the standard range (\code{fullRange}), or on a user defined range \code{fitRange}.
\item Estimate of the the Fractal Dimension (\code{FD}) using conversion formula's reported in Hasselman(2013).
\item Information output by various functions.
}
}
\description{
fd_dfa
}
\examples{

set.seed(1234)

# Brownian noise
fd_dfa(cumsum(rnorm(512)))

# Brownian noise with overlapping bins
fd_dfa(cumsum(rnorm(512)), overlap = 0.5)

# Brownian noise to white noise - windowed analysis
y <- rnorm(1024)
y[1:512] <- cumsum(y[1:512])

id <- ts_windower(y, win = 256, step = 1)

DFAseries <- plyr::ldply(id, function(w){
fd <- fd_dfa(y[w], silent = TRUE)
return(fd$fitRange$FD)
})

op <- par(mfrow=c(2,1))
plot(ts(y))
plot(ts(DFAseries[,2]))
lines(c(0,770),c(1.5,1.5), col = "red3")
lines(c(0,770),c(1.1,1.1), col = "steelblue")
par(op)

}
\references{
Hasselman, F. (2013). When the blind curve is finite: dimension estimation and model inference based on empirical waveforms. Frontiers in Physiology, 4, 75. https://doi.org/10.3389/fphys.2013.00075
}
\seealso{
Other Fluctuation Analyses: 
\code{\link{fd_RR}()},
\code{\link{fd_allan}()},
\code{\link{fd_mfdfa}()},
\code{\link{fd_psd}()},
\code{\link{fd_sda}()},
\code{\link{fd_sev}()}
}
\author{
Fred Hasselman
}
\concept{Fluctuation Analyses}
